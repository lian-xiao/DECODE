"""
å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ç»“æœæ±‡æ€»å·¥å…·
ç”¨äºæœé›†å’Œæ±‡æ€»æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
"""

import os
import sys
import logging
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional, Union
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiModalResultCollector:
    """å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ç»“æœæ±‡æ€»å™¨"""
    
    @staticmethod
    def collect_and_summarize_results(
        results_dir: str, 
        output_dir: str = None, 
        model_pattern: str = "*",
        task_types: List[str] = None
    ) -> Dict[str, Path]:
        """
        æœé›†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶å¹¶è¿›è¡Œæ±‡æ€»
        
        Args:
            results_dir: ç»“æœç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨results_dirï¼‰
            model_pattern: æ¨¡å‹æ–‡ä»¶åŒ¹é…æ¨¡å¼
            task_types: è¦æœé›†çš„ä»»åŠ¡ç±»å‹ï¼Œå¦‚['moa_results', 'rna_reconstruction', 'pheno_reconstruction', 'summary']
            
        Returns:
            Dict[str, Path]: æ±‡æ€»åçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        
        results_dir = Path(results_dir)
        if output_dir is None:
            output_dir = results_dir
        else:
            output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if not results_dir.exists():
            logger.error(f"Results directory does not exist: {results_dir}")
            return {}
        
        logger.info(f"ğŸ” Collecting results from: {results_dir}")
        logger.info(f"ğŸ“ Output directory: {output_dir}")
        
        # é»˜è®¤æœé›†çš„æ–‡ä»¶ç±»å‹
        if task_types is None:
            task_types = ['moa_results', 'rna_reconstruction', 'pheno_reconstruction', 'summary']
        
        # æœé›†ä¸åŒç±»å‹çš„æ–‡ä»¶
        file_patterns = {
            'moa_results': f"{model_pattern}_moa_results_*.csv.gz",
            'rna_reconstruction': f"{model_pattern}_rna_reconstruction_results_*.csv.gz", 
            'pheno_reconstruction': f"{model_pattern}_pheno_reconstruction_results_*.csv.gz",
            'summary': f"{model_pattern}_summary_*.csv",
            'unified_summary': f"{model_pattern}_unified_summary_*.csv"
        }
        
        # åªæœé›†æŒ‡å®šçš„ä»»åŠ¡ç±»å‹
        selected_patterns = {k: v for k, v in file_patterns.items() if k in task_types}
        
        collected_files = {}
        
        for file_type, pattern in selected_patterns.items():
            files = list(results_dir.rglob(pattern))  # é€’å½’æœç´¢
            if files:
                collected_files[file_type] = files
                logger.info(f"Found {len(files)} {file_type} files")
            else:
                logger.warning(f"No {file_type} files found with pattern: {pattern}")
        
        if not collected_files:
            logger.warning("No result files found to process")
            return {}
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆå¹¶å„ç±»å‹çš„æ–‡ä»¶
        combined_files = {}
        
        for file_type, files in collected_files.items():
            logger.info(f"Processing {file_type} files...")
            
            if file_type in ['summary', 'unified_summary']:
                # summaryæ–‡ä»¶ä½¿ç”¨CSVæ ¼å¼
                combined_file = output_dir / f"collected_{file_type}_{timestamp}.csv"
                MultiModalResultCollector._combine_collected_files(files, combined_file, is_summary=True)
            else:
                # å…¶ä»–æ–‡ä»¶ä½¿ç”¨å‹ç¼©æ ¼å¼
                combined_file = output_dir / f"collected_{file_type}_{timestamp}.csv.gz"
                MultiModalResultCollector._combine_collected_files(files, combined_file, is_summary=False)
            
            combined_files[file_type] = combined_file
        
        # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡æŠ¥å‘Š
        summary_file = None
        for file_type in ['unified_summary', 'summary']:
            if file_type in combined_files:
                summary_file = combined_files[file_type]
                break
        
        if summary_file:
            analysis_report = MultiModalResultCollector._create_collection_analysis_report(
                summary_file, output_dir, timestamp
            )
            combined_files['analysis_report'] = analysis_report
        
        # åˆ›å»ºæ–‡ä»¶æ¸…å•
        manifest_file = MultiModalResultCollector._create_collection_manifest(
            results_dir, collected_files, combined_files, output_dir, timestamp
        )
        combined_files['manifest'] = manifest_file
        
        logger.info("âœ… Results collection and summarization completed!")
        logger.info("ğŸ“„ Generated files:")
        for file_type, file_path in combined_files.items():
            if file_path.exists():
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                logger.info(f"  {file_type}: {file_path.name} ({file_size_mb:.2f} MB)")
        
        return combined_files
    
    @staticmethod
    def _combine_collected_files(file_paths: List[Path], output_path: Path, is_summary: bool = False):
        """åˆå¹¶æœé›†åˆ°çš„æ–‡ä»¶"""
        
        combined_dfs = []
        total_records = 0
        
        for file_path in file_paths:
            try:
                # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
                if str(file_path).endswith('.gz'):
                    df = pd.read_csv(file_path, compression='gzip')
                else:
                    df = pd.read_csv(file_path)
                
                # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
                df['source_file'] = file_path.name
                df['source_dir'] = str(file_path.parent.relative_to(file_path.parents[1] if len(file_path.parents) > 1 else file_path.parent))
                
                combined_dfs.append(df)
                total_records += len(df)
                logger.debug(f"Loaded {len(df)} records from {file_path}")
                
            except Exception as e:
                logger.warning(f"Failed to load {file_path}: {e}")
        
        if combined_dfs:
            combined_df = pd.concat(combined_dfs, ignore_index=True)
            
            # æ ¹æ®ç±»å‹é€‰æ‹©ä¿å­˜æ–¹å¼
            if is_summary:
                combined_df.to_csv(output_path, index=False)
            else:
                combined_df.to_csv(output_path, index=False, compression='gzip')
            
            file_size_mb = output_path.stat().st_size / (1024 * 1024)
            logger.info(f"Combined {len(file_paths)} files into {output_path.name}: {len(combined_df)} records, {file_size_mb:.2f} MB")
        else:
            logger.warning("No valid data found to combine")
    
    @staticmethod
    def _create_collection_analysis_report(summary_file: Path, output_dir: Path, timestamp: str) -> Path:
        """ä¸ºæœé›†çš„ç»“æœåˆ›å»ºåˆ†ææŠ¥å‘Š"""
        
        report_file = output_dir / f"collection_analysis_report_{timestamp}.txt"
        
        try:
            df = pd.read_csv(summary_file)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 100 + "\n")
                f.write("COLLECTED MULTIMODAL RESULTS ANALYSIS REPORT\n")
                f.write("=" * 100 + "\n")
                f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Source: {summary_file.name}\n")
                f.write(f"Total Records: {len(df)}\n\n")
                
                # æ•°æ®æºåˆ†æ
                if 'source_file' in df.columns:
                    unique_sources = df['source_file'].nunique()
                    f.write(f"ğŸ“ DATA SOURCES:\n")
                    f.write(f"Unique source files: {unique_sources}\n")
                    
                    source_counts = df['source_file'].value_counts()
                    f.write("Records per source file:\n")
                    for source, count in source_counts.head(10).items():
                        f.write(f"  {source}: {count} records\n")
                    if len(source_counts) > 10:
                        f.write(f"  ... and {len(source_counts) - 10} more files\n")
                    f.write("\n")
                
                # å®éªŒé…ç½®åˆ†æ
                config_columns = ['strategy', 'scenario', 'fold', 'seed']
                available_configs = [col for col in config_columns if col in df.columns]
                
                if available_configs:
                    f.write("ğŸ”§ EXPERIMENT CONFIGURATIONS:\n")
                    for config in available_configs:
                        unique_values = df[config].unique()
                        f.write(f"{config.capitalize()}: {len(unique_values)} unique values {list(unique_values)}\n")
                    f.write("\n")
                
                # æ€§èƒ½ç»Ÿè®¡
                performance_columns = ['moa_accuracy', 'moa_f1_macro', 'moa_f1_weighted', 'rna_r2', 'pheno_r2', 'rna_mse', 'pheno_mse']
                available_performance = [col for col in performance_columns if col in df.columns]
                
                if available_performance:
                    f.write("ğŸ“Š PERFORMANCE STATISTICS:\n")
                    for metric in available_performance:
                        valid_data = df[df[metric].notna()]
                        if len(valid_data) > 0:
                            mean_val = valid_data[metric].mean()
                            std_val = valid_data[metric].std()
                            min_val = valid_data[metric].min()
                            max_val = valid_data[metric].max()
                            f.write(f"{metric}:\n")
                            f.write(f"  Mean: {mean_val:.4f} Â± {std_val:.4f}\n")
                            f.write(f"  Range: [{min_val:.4f}, {max_val:.4f}]\n")
                            f.write(f"  Valid records: {len(valid_data)}/{len(df)}\n")
                    f.write("\n")
                
                # ç­–ç•¥å¯¹æ¯”ï¼ˆå¦‚æœæœ‰å¤šä¸ªç­–ç•¥ï¼‰
                if 'strategy' in df.columns and len(df['strategy'].unique()) > 1:
                    f.write("ğŸ“ˆ STRATEGY COMPARISON:\n")
                    
                    # MOAå‡†ç¡®ç‡å¯¹æ¯”
                    if 'moa_accuracy' in df.columns:
                        strategy_stats = df.groupby('strategy')['moa_accuracy'].agg(['mean', 'std', 'count']).round(4)
                        f.write("MOA Accuracy by Strategy:\n")
                        for strategy, stats in strategy_stats.iterrows():
                            f.write(f"  {strategy}: {stats['mean']:.4f} Â± {stats['std']:.4f} ({stats['count']} experiments)\n")
                        f.write("\n")
                    
                    # é‡å»ºæ€§èƒ½å¯¹æ¯”
                    reconstruction_metrics = ['rna_r2', 'pheno_r2']
                    for metric in reconstruction_metrics:
                        if metric in df.columns:
                            valid_data = df[df[metric].notna() & (df[metric] != 0)]
                            if len(valid_data) > 0:
                                strategy_stats = valid_data.groupby('strategy')[metric].agg(['mean', 'std', 'count']).round(4)
                                f.write(f"{metric.replace('_', ' ').title()} by Strategy:\n")
                                for strategy, stats in strategy_stats.iterrows():
                                    f.write(f"  {strategy}: {stats['mean']:.4f} Â± {stats['std']:.4f} ({stats['count']} experiments)\n")
                                f.write("\n")
                
                # åœºæ™¯å¯¹æ¯”ï¼ˆå¦‚æœæœ‰å¤šä¸ªåœºæ™¯ï¼‰
                if 'scenario' in df.columns and len(df['scenario'].unique()) > 1:
                    f.write("ğŸ§¬ SCENARIO COMPARISON:\n")
                    
                    scenario_names = {
                        'no_missing': 'No Missing',
                        'pheno_missing': 'Pheno Missing',
                        'rna_missing': 'RNA Missing',
                        'both_missing': 'Both Missing'
                    }
                    
                    if 'moa_accuracy' in df.columns:
                        scenario_stats = df.groupby('scenario')['moa_accuracy'].agg(['mean', 'std', 'count']).round(4)
                        f.write("MOA Accuracy by Scenario:\n")
                        for scenario, stats in scenario_stats.iterrows():
                            scenario_display = scenario_names.get(scenario, scenario)
                            f.write(f"  {scenario_display}: {stats['mean']:.4f} Â± {stats['std']:.4f} ({stats['count']} experiments)\n")
                        f.write("\n")
                
                # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                f.write("ğŸ” DATA INTEGRITY CHECK:\n")
                f.write(f"Total rows: {len(df)}\n")
                f.write(f"Complete rows (no NaN): {len(df.dropna())}\n")
                
                missing_data = df.isnull().sum()
                if missing_data.sum() > 0:
                    f.write("Missing data summary:\n")
                    for col, missing_count in missing_data[missing_data > 0].items():
                        missing_pct = (missing_count / len(df)) * 100
                        f.write(f"  {col}: {missing_count} ({missing_pct:.1f}%)\n")
                
                # æœ€ä½³æ€§èƒ½æ€»ç»“
                f.write("\nğŸ† BEST PERFORMANCE SUMMARY:\n")
                if 'moa_accuracy' in df.columns:
                    best_exp = df.loc[df['moa_accuracy'].idxmax()]
                    f.write(f"Best MOA Accuracy: {best_exp['moa_accuracy']:.4f}")
                    if 'strategy' in best_exp and 'scenario' in best_exp:
                        f.write(f" ({best_exp['strategy']}-{best_exp['scenario']})")
                    if 'fold' in best_exp:
                        f.write(f" fold {best_exp['fold']}")
                    f.write("\n")
                
                if 'rna_r2' in df.columns:
                    valid_rna = df[df['rna_r2'].notna() & (df['rna_r2'] != 0)]
                    if len(valid_rna) > 0:
                        best_rna = valid_rna.loc[valid_rna['rna_r2'].idxmax()]
                        f.write(f"Best RNA RÂ²: {best_rna['rna_r2']:.4f}")
                        if 'strategy' in best_rna and 'scenario' in best_rna:
                            f.write(f" ({best_rna['strategy']}-{best_rna['scenario']})")
                        f.write("\n")
                
                if 'pheno_r2' in df.columns:
                    valid_pheno = df[df['pheno_r2'].notna() & (df['pheno_r2'] != 0)]
                    if len(valid_pheno) > 0:
                        best_pheno = valid_pheno.loc[valid_pheno['pheno_r2'].idxmax()]
                        f.write(f"Best Pheno RÂ²: {best_pheno['pheno_r2']:.4f}")
                        if 'strategy' in best_pheno and 'scenario' in best_pheno:
                            f.write(f" ({best_pheno['strategy']}-{best_pheno['scenario']})")
                        f.write("\n")
                
                f.write("\n" + "=" * 100 + "\n")
                
        except Exception as e:
            logger.error(f"Failed to create collection analysis report: {e}")
            with open(report_file, 'w') as f:
                f.write(f"Collection Analysis Report Generation Failed\n")
                f.write(f"Error: {e}\n")
        
        return report_file
    
    @staticmethod
    def _create_collection_manifest(
        source_dir: Path, 
        collected_files: Dict[str, List[Path]], 
        combined_files: Dict[str, Path],
        output_dir: Path, 
        timestamp: str
    ) -> Path:
        """åˆ›å»ºæœé›†ç»“æœçš„æ¸…å•æ–‡ä»¶"""
        
        manifest_file = output_dir / f"collection_manifest_{timestamp}.txt"
        
        with open(manifest_file, 'w', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write("MULTIMODAL RESULTS COLLECTION MANIFEST\n")
            f.write("=" * 100 + "\n")
            f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Source directory: {source_dir}\n")
            f.write(f"Output directory: {output_dir}\n\n")
            
            # åŸå§‹æ–‡ä»¶æ¸…å•
            f.write("ğŸ“‚ ORIGINAL FILES COLLECTED:\n")
            f.write("-" * 50 + "\n")
            
            total_original_files = 0
            total_original_size = 0
            
            for file_type, files in collected_files.items():
                f.write(f"\n{file_type.upper()} FILES ({len(files)} files):\n")
                
                type_size = 0
                for file_path in files:
                    if file_path.exists():
                        file_size = file_path.stat().st_size
                        file_size_mb = file_size / (1024 * 1024)
                        type_size += file_size
                        f.write(f"  ğŸ“„ {file_path.name} ({file_size_mb:.2f} MB)\n")
                        try:
                            f.write(f"      Path: {file_path.relative_to(source_dir)}\n")
                        except ValueError:
                            f.write(f"      Path: {file_path}\n")
                
                type_size_mb = type_size / (1024 * 1024)
                f.write(f"  Subtotal: {len(files)} files, {type_size_mb:.2f} MB\n")
                
                total_original_files += len(files)
                total_original_size += type_size
            
            total_original_size_mb = total_original_size / (1024 * 1024)
            f.write(f"\nTOTAL ORIGINAL: {total_original_files} files, {total_original_size_mb:.2f} MB\n")
            
            # åˆå¹¶åçš„æ–‡ä»¶æ¸…å•
            f.write("\nğŸ“¦ COMBINED OUTPUT FILES:\n")
            f.write("-" * 50 + "\n")
            
            total_combined_size = 0
            for file_type, file_path in combined_files.items():
                if file_path.exists():
                    file_size = file_path.stat().st_size
                    file_size_mb = file_size / (1024 * 1024)
                    total_combined_size += file_size
                    f.write(f"ğŸ“„ {file_type}: {file_path.name} ({file_size_mb:.2f} MB)\n")
            
            total_combined_size_mb = total_combined_size / (1024 * 1024)
            f.write(f"\nTOTAL COMBINED: {len(combined_files)} files, {total_combined_size_mb:.2f} MB\n")
            
            # å‹ç¼©æ•ˆæœ
            if total_original_size > 0:
                compression_ratio = (1 - total_combined_size / total_original_size) * 100
                f.write(f"\nğŸ’¾ STORAGE EFFICIENCY:\n")
                f.write(f"Space saved: {compression_ratio:.1f}%\n")
                f.write(f"Size reduction: {total_original_size_mb - total_combined_size_mb:.2f} MB\n")
            
            f.write("\nğŸ’¡ USAGE NOTES:\n")
            f.write("- Combined files include source tracking information\n")
            f.write("- Use 'source_file' and 'source_dir' columns to trace data origin\n")
            f.write("- .csv.gz files can be read directly with pandas\n")
            f.write("- Analysis report contains detailed performance statistics\n")
            f.write("- Files from different experiments are automatically merged\n")
            f.write("- Duplicates are preserved to maintain experiment integrity\n")
            
            f.write("\nğŸ“š FILE TYPE DESCRIPTIONS:\n")
            f.write("- moa_results: Detailed MOA classification predictions per sample\n")
            f.write("- rna_reconstruction: RNA feature reconstruction results\n")
            f.write("- pheno_reconstruction: Phenotype feature reconstruction results\n")
            f.write("- summary/unified_summary: Aggregated performance metrics per experiment\n")
            
            f.write("\n" + "=" * 100 + "\n")
        
        return manifest_file
    
    @staticmethod
    def combine_reconstruction_results(
        results_dir: str,
        output_dir: str = None,
        model_pattern: str = "*"
    ) -> Path:
        """
        æ•´åˆæ‰€æœ‰RNAé‡å»ºå’Œè¡¨å‹é‡å»ºçš„ç»“æœæ–‡ä»¶ä¸ºå•ä¸ªCSV.gzæ–‡ä»¶
        
        Args:
            results_dir: ç»“æœç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨results_dirï¼‰
            model_pattern: æ¨¡å‹æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            Path: æ•´åˆåçš„æ–‡ä»¶è·¯å¾„
        """
        
        results_dir = Path(results_dir)
        if output_dir is None:
            output_dir = results_dir
        else:
            output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ”— Combining reconstruction results from: {results_dir}")
        
        # æœé›†RNAå’Œè¡¨å‹é‡å»ºæ–‡ä»¶
        reconstruction_patterns = {
            'rna_reconstruction': f"{model_pattern}_rna_reconstruction_results_*.csv.gz",
            'pheno_reconstruction': f"{model_pattern}_pheno_reconstruction_results_*.csv.gz"
        }
        
        all_reconstruction_files = []
        
        for recon_type, pattern in reconstruction_patterns.items():
            files = list(results_dir.rglob(pattern))
            if files:
                logger.info(f"Found {len(files)} {recon_type} files")
                all_reconstruction_files.extend(files)
            else:
                logger.warning(f"No {recon_type} files found with pattern: {pattern}")
        
        if not all_reconstruction_files:
            logger.warning("No reconstruction files found to combine")
            return None
        
        # åˆå¹¶æ‰€æœ‰é‡å»ºç»“æœ
        combined_dfs = []
        total_records = 0
        
        for file_path in all_reconstruction_files:
            try:
                df = pd.read_csv(file_path, compression='gzip')
                
                # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
                df['source_file'] = file_path.name
                df['source_dir'] = str(file_path.parent.relative_to(file_path.parents[1] if len(file_path.parents) > 1 else file_path.parent))
                
                # ä»æ–‡ä»¶åæ¨æ–­é‡å»ºç±»å‹
                if 'rna_reconstruction' in file_path.name:
                    df['reconstruction_type'] = 'rna'
                elif 'pheno_reconstruction' in file_path.name:
                    df['reconstruction_type'] = 'pheno'
                else:
                    df['reconstruction_type'] = 'unknown'
                
                combined_dfs.append(df)
                total_records += len(df)
                logger.debug(f"Loaded {len(df)} records from {file_path}")
                
            except Exception as e:
                logger.warning(f"Failed to load {file_path}: {e}")
        
        if not combined_dfs:
            logger.warning("No valid reconstruction data found to combine")
            return None
        
        # åˆå¹¶æ•°æ®
        combined_df = pd.concat(combined_dfs, ignore_index=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"combined_all_reconstruction_results_{timestamp}.csv.gz"
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
        combined_df.to_csv(output_file, index=False, compression='gzip')
        
        file_size_mb = output_file.stat().st_size / (1024 * 1024)
        logger.info(f"âœ… Combined {len(all_reconstruction_files)} reconstruction files into {output_file.name}")
        logger.info(f"   Total records: {len(combined_df)}, File size: {file_size_mb:.2f} MB")
        logger.info(f"   RNA records: {len(combined_df[combined_df['reconstruction_type'] == 'rna'])}")
        logger.info(f"   Pheno records: {len(combined_df[combined_df['reconstruction_type'] == 'pheno'])}")
        
        return output_file


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ±‡æ€»å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ç»“æœæ–‡ä»¶')
    parser.add_argument('--results_dir', type=str, default="results/multimodal_LINCS_evaluation/MultiModalMOAPredictor",
                       help='ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨results_dirï¼‰')
    parser.add_argument('--model_pattern', type=str, default='*',
                       help='æ¨¡å‹æ–‡ä»¶åŒ¹é…æ¨¡å¼')
    parser.add_argument('--task_types', nargs='+',
                       default=['rna_reconstruction', 'pheno_reconstruction'],
                       help='è¦æœé›†çš„ä»»åŠ¡ç±»å‹')#'summary'moa_results', 
    parser.add_argument('--combine_reconstruction', action='store_true',
                       help='åŒæ—¶æ•´åˆæ‰€æœ‰é‡å»ºç»“æœæ–‡ä»¶')
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¯åŠ¨å¤šæ¨¡æ€ç»“æœæ±‡æ€»å™¨...")
    
    # æ‰§è¡Œæ±‡æ€»
    result_files = MultiModalResultCollector.collect_and_summarize_results(
        results_dir=args.results_dir,
        output_dir=args.output_dir,
        model_pattern=args.model_pattern,
        task_types=args.task_types
    )
    
    # å¦‚æœæŒ‡å®šäº†æ•´åˆé‡å»ºç»“æœ
    if args.combine_reconstruction:
        logger.info("ğŸ”— æ­£åœ¨æ•´åˆé‡å»ºç»“æœæ–‡ä»¶...")
        combined_reconstruction_file = MultiModalResultCollector.combine_reconstruction_results(
            results_dir=args.results_dir,
            output_dir=args.output_dir,
            model_pattern=args.model_pattern
        )
        if combined_reconstruction_file:
            result_files['combined_reconstruction'] = combined_reconstruction_file
    
    # è¾“å‡ºç»“æœ
    if result_files:
        logger.info("\n" + "=" * 80)
        logger.info("âœ… ç»“æœæ±‡æ€»å®Œæˆ!")
        logger.info("=" * 80)
        for file_type, file_path in result_files.items():
            logger.info(f"ğŸ“„ {file_type}: {file_path}")
        
        logger.info("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        logger.info("1. æŸ¥çœ‹analysis_reportäº†è§£æ•°æ®æ±‡æ€»ç»Ÿè®¡")
        logger.info("2. ä½¿ç”¨pd.read_csv()åŠ è½½æ±‡æ€»åçš„CSVæ–‡ä»¶")
        logger.info("3. æŸ¥çœ‹manifestäº†è§£æ–‡ä»¶æ¥æºå’Œç»„ç»‡ç»“æ„")
        if args.combine_reconstruction:
            logger.info("4. combined_reconstructionåŒ…å«æ‰€æœ‰RNAå’Œè¡¨å‹é‡å»ºç»“æœ")
        logger.info("=" * 80)
    else:
        logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶è¿›è¡Œæ±‡æ€»")


if __name__ == '__main__':
    main()