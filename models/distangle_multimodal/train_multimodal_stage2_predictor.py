"""
Multimodal MOA prediction model training script - Stage 2
"""

import os
import sys
import argparse
import yaml
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Callable

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from models.distangle_multimodal.distangle_multimodal_moa_predictor import MultiModalMOAPredictor
from utils.metrics import create_metrics_calculator
from DModule.datamodule import MMDPDataModule

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration file"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config



class MOADataModule(MMDPDataModule):
    """MOA data module with multi-label support"""
    
    def __init__(
        self,
        data_dir: str,
        dataset_name: str = "dataset",
        batch_size: int = 32,
        num_workers: int = 4,
        pin_memory: bool = True,
        train_split: float = 0.7,
        val_split: float = 0.1,
        test_split: float = 0.2,
        preload_features: bool = True,
        preload_metadata: bool = True,
        return_metadata: bool = True,
        feature_groups_only: Optional[List[int]] = None,
        metadata_columns_only: Optional[List[str]] = None,
        device: str = 'cpu',
        moa_column: str = 'Metadata_moa',
        save_label_encoder: bool = True,
        # MOAé¢„æµ‹æ¨¡å‹ç‰¹å®šçš„ç‰¹å¾ç»„æ˜ å°„
        feature_group_mapping: Optional[Dict[int, str]] = None,
        # å½’ä¸€åŒ–ç›¸å…³å‚æ•°
        normalize_features: bool = False,
        normalization_method: str = 'standardize',
        exclude_modalities: Optional[List[str]] = None,
        save_scalers: bool = True,
        # å¤šæ ‡ç­¾åˆ†ç±»å‚æ•°
        multi_label_classification: bool = False,
        label_separator: str = '|',
        min_label_frequency: int = 1,
        **kwargs
    ):
        """Initialize MOA data module"""
        
        # è®¾ç½®MOAé¢„æµ‹æ¨¡å‹çš„é»˜è®¤ç‰¹å¾ç»„æ˜ å°„
        if feature_group_mapping is None:
            feature_group_mapping = {
                0: 'pheno',    # è¡¨å‹æ•°æ®
                1: 'rna',      # RNAè¡¨è¾¾æ•°æ®
                2: 'drug',     # è¯ç‰©ç‰¹å¾
                3: 'dose'      # å‰‚é‡ä¿¡æ¯
            }
        
        # è®¾ç½®é»˜è®¤çš„å…ƒæ•°æ®åˆ—ï¼ˆåŒ…å«MOAä¿¡æ¯ï¼‰
        if metadata_columns_only is None:
            metadata_columns_only = [moa_column, 'Metadata_broad_sample', 'Metadata_pert_id']
        
        # ä¿å­˜å¤šæ ‡ç­¾åˆ†ç±»å‚æ•°
        self.multi_label_classification = multi_label_classification
        self.label_separator = label_separator
        self.min_label_frequency = min_label_frequency
        
        super().__init__(
            data_dir=data_dir,
            dataset_name=dataset_name,
            batch_size=batch_size,
            num_workers=num_workers,
            pin_memory=pin_memory,
            train_split=train_split,
            val_split=val_split,
            test_split=test_split,
            preload_features=preload_features,
            preload_metadata=preload_metadata,
            return_metadata=return_metadata,
            feature_groups_only=feature_groups_only,
            metadata_columns_only=metadata_columns_only,
            device=device,
            moa_column=moa_column,
            save_label_encoder=save_label_encoder,
            feature_group_mapping=feature_group_mapping,
            normalize_features=normalize_features,
            normalization_method=normalization_method,
            exclude_modalities=exclude_modalities,
            save_scalers=save_scalers,
            **kwargs
        )
        
        logger.info(f"MOADataModule initialized:")
        logger.info(f"  Multi-label classification: {self.multi_label_classification}")
    
    def setup(self, stage: str = None, split_index: int = 0, **kwargs):
        """Setup data module with MOA label processing"""
        super().setup(stage=stage, split_index=split_index, **kwargs)
        
        if self.multi_label_classification:
            logger.info("Processing multi-label MOA classification...")
            
            if hasattr(self.full_dataset, 'metadata_df') and self.full_dataset.metadata_df is not None:
                self._setup_multilabel_moa()
                logger.info(f"Multi-label processing completed!")
                logger.info(f"  Number of classes: {self.num_classes}")
            else:
                logger.warning("No metadata found for multi-label processing")
    
    def _setup_multilabel_moa(self):
        """Setup multi-label MOA classification"""
        import pandas as pd
        from sklearn.preprocessing import MultiLabelBinarizer
        from collections import Counter
        
        logger.info("Setting up multi-label classification...")
        
        moa_labels = self._extract_metadata_df()[self.moa_column].values
        
        label_lists = []
        for moa_label in moa_labels:
            if pd.isna(moa_label) or moa_label.lower() in ['nan', 'none', '']:
                label_lists.append([])
            else:
                labels = [label.strip() for label in moa_label.split(self.label_separator)]
                labels = [label for label in labels if label and label.lower() != 'nan']
                label_lists.append(labels)
        
        all_labels = [label for labels in label_lists for label in labels]
        label_counts = Counter(all_labels)
        
        frequent_labels = {label for label, count in label_counts.items() 
                          if count >= self.min_label_frequency}
        
        logger.info(f"Found {len(frequent_labels)} frequent labels (min frequency: {self.min_label_frequency})")
        
        filtered_label_lists = []
        for labels in label_lists:
            filtered_labels = [label for label in labels if label in frequent_labels]
            filtered_label_lists.append(filtered_labels)
        
        mlb = MultiLabelBinarizer(classes=sorted(frequent_labels))
        moa_matrix = mlb.fit_transform(filtered_label_lists)
        
        self.moa_to_idx = {label: i for i, label in enumerate(mlb.classes_)}
        self.idx_to_moa = {i: label for label, i in self.moa_to_idx.items()}
        self.unique_moas = list(mlb.classes_)
        self.num_classes = len(mlb.classes_)
        self.mlb = mlb
        self.moa_matrix = moa_matrix
        
        logger.info(f"Average labels per sample: {moa_matrix.sum(axis=1).mean():.2f}")
        
        samples_with_no_labels = (moa_matrix.sum(axis=1) == 0).sum()
        if samples_with_no_labels > 0:
            logger.warning(f"Warning: {samples_with_no_labels} samples have no labels")
    
    def convert_batch_to_moa_format(self, batch: Dict) -> Dict[str, torch.Tensor]:
        """å°†æ‰¹æ¬¡è½¬æ¢ä¸ºMOAé¢„æµ‹æ¨¡å‹æœŸæœ›çš„æ ¼å¼"""
        # ä½¿ç”¨çˆ¶ç±»çš„è½¬æ¢æ–¹æ³•
        moa_batch = self.convert_batch_to_mmdp_format(batch)
                                                 
        # ç¡®ä¿å¿…è¦çš„æ¨¡æ€å­˜åœ¨ï¼Œå¦‚æœç¼ºå¤±åˆ™ç”¨é›¶å¼ é‡å¡«å……
        device = next(iter(moa_batch.values())).device if moa_batch else torch.device('cpu')
        batch_size = next(iter(moa_batch.values())).size(0) if moa_batch else 1
        
        # æ£€æŸ¥å’Œè¡¥å……ç¼ºå¤±çš„æ¨¡æ€
        required_modalities = ['drug', 'dose', 'rna', 'pheno']
        default_dims = {
            'drug': 768, 'dose': 1, 'rna': 978, 'pheno': 1783
        }
        
        for modality in required_modalities:
            if modality not in moa_batch:
                dim = default_dims.get(modality, 100)
                moa_batch[modality] = torch.zeros(batch_size, dim, device=device)
        
        return moa_batch
    
    def _convert_moa_to_multilabel_format(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """Convert MOA labels to multi-label format"""
        import torch
        
        if 'metadata' in batch and isinstance(batch['metadata'], list):
            batch_size = len(batch['metadata'])
            multilabel_matrix = torch.zeros(batch_size, self.num_classes)
            
            for i, metadata_dict in enumerate(batch['metadata']):
                if self.moa_column in metadata_dict:
                    moa_string = str(metadata_dict[self.moa_column])
                    
                    if moa_string and moa_string.lower() not in ['nan', 'none', '']:
                        labels = [label.strip() for label in moa_string.split(self.label_separator)]
                        labels = [label for label in labels if label and label.lower() != 'nan']
                        
                        for label in labels:
                            if label in self.moa_to_idx:
                                class_idx = self.moa_to_idx[label]
                                multilabel_matrix[i, class_idx] = 1.0
            
            batch['moa'] = multilabel_matrix
        
        elif 'moa' in batch and batch['moa'].dim() == 1:
            batch_size = batch['moa'].size(0)
            multilabel_matrix = torch.zeros(batch_size, self.num_classes)
            
            for i, class_idx in enumerate(batch['moa']):
                if 0 <= class_idx < self.num_classes:
                    multilabel_matrix[i, class_idx] = 1.0
            
            batch['moa'] = multilabel_matrix
        
        return batch
    
    def create_dataloader_with_moa_transform(
        self,
        dataset,
        batch_size: Optional[int] = None,
        shuffle: bool = False,
        **kwargs
    ) -> torch.utils.data.DataLoader:
        """åˆ›å»ºä¸“é—¨ä¸ºMOAé¢„æµ‹æ¨¡å‹è®¾è®¡çš„DataLoader"""
        if batch_size is None:
            batch_size = self.batch_size
        
        def moa_collate_fn(batch):
            from DModule.datamodule import custom_collate_fn
            collated_batch = custom_collate_fn(batch)
            moa_batch = self.convert_batch_to_moa_format(collated_batch)
            
            # å¦‚æœæ˜¯å¤šæ ‡ç­¾åˆ†ç±»ï¼Œéœ€è¦è½¬æ¢MOAæ ‡ç­¾æ ¼å¼
            if self.multi_label_classification:
                # ç¡®ä¿æˆ‘ä»¬æœ‰å¿…è¦çš„å±æ€§è¿›è¡Œå¤šæ ‡ç­¾è½¬æ¢
                if hasattr(self, 'moa_to_idx') and hasattr(self, 'num_classes'):
                    moa_batch = self._convert_moa_to_multilabel_format(moa_batch)
                else:
                    logger.warning("âš ï¸ Multi-label classification enabled but MOA mapping not found. "
                                 "Make sure setup() was called properly.")
            
            return moa_batch
        
        return torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            collate_fn=moa_collate_fn,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory and torch.cuda.is_available(),
            **kwargs
        )
    
    def train_dataloader(self) -> torch.utils.data.DataLoader:
        return self.create_dataloader_with_moa_transform(
            self.train_dataset, shuffle=True, drop_last=True)
    
    def val_dataloader(self) -> torch.utils.data.DataLoader:
        return self.create_dataloader_with_moa_transform(
            self.val_dataset, shuffle=False, drop_last=False)
    
    def test_dataloader(self) -> torch.utils.data.DataLoader:
        return self.create_dataloader_with_moa_transform(
            self.test_dataset, shuffle=False, drop_last=False)
    
    def predict_dataloader(self) -> torch.utils.data.DataLoader:
        predict_dataset = getattr(self, 'predict_dataset', self.test_dataset)
        return self.create_dataloader_with_moa_transform(
            predict_dataset, shuffle=False, drop_last=False)
    
    def get_model_input_dims(self) -> Dict[str, int]:
        """è·å–æ¨¡å‹è¾“å…¥ç»´åº¦ä¿¡æ¯"""
        data_info = self.get_data_info()
        data_dims = data_info.get('data_dims', {})
        
        return {
            'drug_dim': data_dims.get('drug', 768),
            'dose_dim': data_dims.get('dose', 1),
            'rna_dim': data_dims.get('rna', 978),
            'pheno_dim': data_dims.get('pheno', 1783),
            'num_moa_classes': self.num_classes or 12
        }
    
    def get_moa_info(self) -> Dict[str, Any]:
        """è·å–MOAç›¸å…³ä¿¡æ¯"""
        return {
            'num_classes': self.num_classes,
            'unique_moas': self.unique_moas,
            'moa_to_idx': self.moa_to_idx,
            'idx_to_moa': self.idx_to_moa,
            'moa_column': self.moa_column,
        }

def create_model(config: Dict[str, Any], data_module: MOADataModule) -> MultiModalMOAPredictor:
    """Create model with optional Stage 1 checkpoint loading"""
    
    model_config = config['model_config'].copy()
    
    data_config = config.get('data', {})
    if 'multi_label_classification' in data_config:
        model_config['multi_label_classification'] = data_config['multi_label_classification']
    
    model_dims = data_module.get_model_input_dims()
    model_config.update(model_dims)
    
    # æ·»åŠ MOAç±»åˆ«åç§°
    if 'moa_class_names' in config.get('data', {}):
        model_config['moa_class_names'] = config['data']['moa_class_names']
    else:
        # ä»æ•°æ®æ¨¡å—è·å–MOAç±»åˆ«åç§°
        moa_info = data_module.get_moa_info()
        if moa_info.get('unique_moas'):
            model_config['moa_class_names'] = moa_info['unique_moas']
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼Œéœ€è¦åŠ è½½ç¬¬ä¸€é˜¶æ®µæƒé‡
    stage1_checkpoint_path = config.get('training', {}).get('stage1_checkpoint_path', None)
    
    if stage1_checkpoint_path:
        logger.info(f"ğŸ”„ STAGE 2 TRAINING - Loading Stage 1 checkpoint")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(stage1_checkpoint_path):
            raise FileNotFoundError(f"Stage 1 checkpoint not found: {stage1_checkpoint_path}")
        
        try:
            model = _load_model_with_flexible_weights(stage1_checkpoint_path, model_config)
            logger.info("âœ… Successfully loaded Stage 1 weights")
            
            # å¯é€‰ï¼šå†»ç»“éƒ¨åˆ†å±‚ï¼ˆå¦‚æœé…ç½®ä¸­æŒ‡å®šï¼‰
            freeze_config = config.get('training', {}).get('freeze_layers', {})
            if freeze_config.get('enabled', False):
                freeze_layers = freeze_config.get('layers', [])
                model = _freeze_model_layers(model, freeze_layers)
                
        except Exception as e:
            logger.error(f"âŒ Failed to load Stage 1 checkpoint: {e}")
            raise RuntimeError(f"Unable to load Stage 1 checkpoint: {e}")
    else:
        model = MultiModalMOAPredictor(**model_config)
        logger.info("ğŸ†• Creating new model")
    
    logger.info("Model created successfully!")
    model_info = model.get_model_info()
    logger.info(f"Total parameters: {model_info['total_parameters']:,}")
    logger.info(f"Trainable parameters: {model_info['trainable_parameters']:,}")
    
    return model


def _load_model_with_flexible_weights(checkpoint_path: str, model_config: Dict[str, Any]) -> MultiModalMOAPredictor:
    """Flexibly load model weights"""
    logger.info("Loading checkpoint with flexible weight loading...")
    
    # åŠ è½½checkpoint
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
    except Exception as e:
        raise RuntimeError(f"Failed to load checkpoint file: {e}")
    
    # è·å–ä¿å­˜çš„çŠ¶æ€å­—å…¸
    if 'state_dict' in checkpoint:
        pretrained_state_dict = checkpoint['state_dict']
        logger.info(f"Loaded state_dict with {len(pretrained_state_dict)} parameters")
    else:
        raise KeyError("No 'state_dict' found in checkpoint")
    
    # å°è¯•ç›´æ¥åŠ è½½ï¼ˆå¦‚æœæ¨¡å‹ç»“æ„å®Œå…¨åŒ¹é…ï¼‰
    try:
        model = MultiModalMOAPredictor.load_from_checkpoint(checkpoint_path, **model_config)
        logger.info("âœ… Direct loading successful")
        return model
    except Exception:
        logger.info("Attempting flexible weight loading...")
    
    # åˆ›å»ºæ–°æ¨¡å‹
    model = MultiModalMOAPredictor(**model_config)
    current_state_dict = model.state_dict()
    
    # çµæ´»åŒ¹é…æƒé‡
    loaded_weights = {}
    missing_weights = []
    incompatible_weights = []
    unexpected_weights = []
    
    # éå†é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•åŒ¹é…åˆ°å½“å‰æ¨¡å‹
    for pretrained_key, pretrained_weight in pretrained_state_dict.items():
        if pretrained_key in current_state_dict:
            current_weight = current_state_dict[pretrained_key]
            
            # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
            if pretrained_weight.shape == current_weight.shape:
                loaded_weights[pretrained_key] = pretrained_weight
                logger.debug(f"âœ… Matched: {pretrained_key} {pretrained_weight.shape}")
            else:
                incompatible_weights.append({
                    'key': pretrained_key,
                    'pretrained_shape': pretrained_weight.shape,
                    'current_shape': current_weight.shape
                })
                logger.debug(f"âŒ Shape mismatch: {pretrained_key} "
                           f"pretrained: {pretrained_weight.shape} vs current: {current_weight.shape}")
        else:
            unexpected_weights.append(pretrained_key)
            logger.debug(f"ğŸ”¶ Unexpected key in pretrained: {pretrained_key}")
    
    # æ£€æŸ¥å½“å‰æ¨¡å‹ä¸­å“ªäº›æƒé‡æ²¡æœ‰è¢«åŠ è½½
    for current_key in current_state_dict.keys():
        if current_key not in loaded_weights:
            missing_weights.append(current_key)
    
    # æ‰“å°åŠ è½½ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"ğŸ“Š Weight Loading Statistics:")
    logger.info(f"  Successfully loaded: {len(loaded_weights)}/{len(current_state_dict)} weights")
    logger.info(f"  Missing weights: {len(missing_weights)}")
    logger.info(f"  Incompatible weights: {len(incompatible_weights)}")
    logger.info(f"  Unexpected weights: {len(unexpected_weights)}")
    
    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    if missing_weights:
        logger.warning(f"âš ï¸ Missing weights (will use random initialization):")
        for key in missing_weights[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.warning(f"    {key}")
        if len(missing_weights) > 10:
            logger.warning(f"    ... and {len(missing_weights) - 10} more")
    
    if incompatible_weights:
        logger.warning(f"âš ï¸ Incompatible weights (shape mismatch):")
        for item in incompatible_weights[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.warning(f"    {item['key']}: {item['pretrained_shape']} -> {item['current_shape']}")
        if len(incompatible_weights) > 5:
            logger.warning(f"    ... and {len(incompatible_weights) - 5} more")
    
    # æ£€æŸ¥æ˜¯å¦åŠ è½½äº†è¶³å¤Ÿçš„æƒé‡
    loading_ratio = len(loaded_weights) / len(current_state_dict)
    logger.info(f"ğŸ“ˆ Weight loading ratio: {loading_ratio:.2%}")
    
    if loading_ratio < 0.5:  # å¦‚æœåŠ è½½çš„æƒé‡å°‘äº50%
        raise RuntimeError(
            f"Critical error: Only {loading_ratio:.2%} of weights were successfully loaded. "
            f"This indicates severe model structure incompatibility. "
            f"Loaded: {len(loaded_weights)}, Expected: {len(current_state_dict)}"
        )
    elif loading_ratio < 0.8:  # å¦‚æœåŠ è½½çš„æƒé‡å°‘äº80%
        logger.warning(
            f"âš ï¸ Warning: Only {loading_ratio:.2%} of weights were loaded. "
            f"Model may not perform optimally."
        )
    
    # åŠ è½½å…¼å®¹çš„æƒé‡
    try:
        model.load_state_dict(loaded_weights, strict=False)
        logger.info("âœ… Successfully loaded compatible weights")
        
        # æä¾›å…³é”®ç»„ä»¶çš„åŠ è½½çŠ¶æ€
        key_components = [
            'drug_encoder', 'rna_encoder', 'pheno_encoder',
            'shared_encoder', 'drug_unique_encoder', 'rna_unique_encoder', 'pheno_unique_encoder',
            'moa_classifier'
        ]
        
        logger.info("ğŸ” Key component loading status:")
        for component in key_components:
            component_loaded = any(component in key for key in loaded_weights.keys())
            status = "âœ…" if component_loaded else "âŒ"
            logger.info(f"  {status} {component}")
            
    except Exception as e:
        raise RuntimeError(f"Failed to load compatible weights into model: {e}")
    
    return model


def _freeze_model_layers(model: MultiModalMOAPredictor, freeze_layers: List[str]) -> MultiModalMOAPredictor:
    """Freeze specified model layers"""
    logger.info(f"Freezing layers: {freeze_layers}")
    
    frozen_count = 0
    total_params = 0
    
    for name, param in model.named_parameters():
        total_params += 1
        
        should_freeze = False
        for layer_name in freeze_layers:
            if layer_name in name:
                should_freeze = True
                break
        
        if should_freeze:
            param.requires_grad = False
            frozen_count += 1
    
    logger.info(f"Frozen {frozen_count}/{total_params} parameters")
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_param_count = sum(p.numel() for p in model.parameters())
    
    logger.info(f"Trainable parameters: {trainable_params:,}")
    logger.info(f"Frozen parameters: {total_param_count - trainable_params:,}")
    
    return model


def create_callbacks(config: Dict[str, Any], output_dir: str, experiment_name: str = '') -> List[pl.Callback]:
    """åˆ›å»ºå›è°ƒå‡½æ•°"""
    
    callbacks = []
    
    # è·å–é…ç½®
    training_config = config.get('training', {})
    early_stopping_config = training_config.get('early_stopping', {})
    checkpoint_config = training_config.get('checkpoint', {})
    model_config = config.get('model_config', {})
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†é˜¶æ®µè®­ç»ƒ
    use_staged_training = model_config.get('use_staged_training', False)
    contrastive_only_epochs = model_config.get('contrastive_only_epochs', 10)
    
    # Early Stoppingå›è°ƒ
    if early_stopping_config.get('monitor'):
        if use_staged_training:
            patients = early_stopping_config.get('patience', 10) + contrastive_only_epochs
        else:
            patients = early_stopping_config.get('patience', 10)
        
        logger.info(f"Early stopping patience: {patients}")

        early_stopping = EarlyStopping(
            monitor=early_stopping_config['monitor'],
            patience=patients,
            mode=early_stopping_config.get('mode', 'max'),
            min_delta=early_stopping_config.get('min_delta', 0.0001),
            verbose=early_stopping_config.get('verbose', True)
        )

        callbacks.append(early_stopping)
        logger.info(f"Early stopping callback added: "
                   f"monitor={early_stopping_config['monitor']}, "
                   f"patience={early_stopping_config.get('patience', 10)}")
    
    # Model Checkpointå›è°ƒ
    if checkpoint_config.get('monitor'):
        checkpoint_dir = Path(output_dir) / 'checkpoints'
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        

        checkpoint = ModelCheckpoint(
            dirpath=checkpoint_dir,
            filename=checkpoint_config.get('filename', 'best-{epoch:02d}-{val_loss:.6f}'),
            monitor=checkpoint_config['monitor'],
            mode=checkpoint_config.get('mode', 'max'),
            save_top_k=checkpoint_config.get('save_top_k', 1),
            save_last=checkpoint_config.get('save_last', True),
            auto_insert_metric_name=checkpoint_config.get('auto_insert_metric_name', False)
        )
        
        callbacks.append(checkpoint)
        logger.info(f"Model checkpoint callback added: "
                   f"monitor={checkpoint_config['monitor']}, "
                   f"save_top_k={checkpoint_config.get('save_top_k', 1)}")
    
    return callbacks

def train_moa_model(config: Dict[str, Any], data_module: MOADataModule, 
                   output_dir: str, experiment_name: str, 
                   callbacks_fn: Optional[Callable] = None) -> Dict[str, Any]:
    """
    è®­ç»ƒMOAé¢„æµ‹æ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
        data_module: æ•°æ®æ¨¡å—
        output_dir: è¾“å‡ºç›®å½•
        experiment_name: å®éªŒåç§°
        callbacks_fn: è‡ªå®šä¹‰å›è°ƒå‡½æ•°ç”Ÿæˆå™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„create_callbacks
        
    Returns:
        è®­ç»ƒç»“æœå­—å…¸
    """
    
    logger.info("Starting MOA prediction model training...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_save_path = output_path / 'config.yaml'
    with open(config_save_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config, data_module)
    
    # æ£€æŸ¥åˆ†é˜¶æ®µè®­ç»ƒé…ç½®
    model_config = config.get('model_config', {})
    use_staged_training = model_config.get('use_staged_training', False)
    contrastive_only_epochs = model_config.get('contrastive_only_epochs', 10)
    
    if use_staged_training:
        logger.info(f"ğŸ¯ STAGED TRAINING ENABLED:")
        logger.info(f"   Contrastive-only epochs: 0-{contrastive_only_epochs-1}")
        logger.info(f"   Task learning epochs: {contrastive_only_epochs}+")
        logger.info(f"   Model configured for staged training")
    else:
        logger.info(f"ğŸ¯ STANDARD TRAINING MODE")
    
    # åˆ›å»ºå›è°ƒ
    if callbacks_fn is not None:
        callbacks = callbacks_fn(config, output_dir, experiment_name)
    else:
        callbacks = create_callbacks(config, output_dir, experiment_name)
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger_tb = TensorBoardLogger(
        save_dir=output_dir,
        name=experiment_name,
        default_hp_metric=False
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    training_config = config['training']
    trainer = pl.Trainer(
        max_epochs=training_config.get('max_epochs', 100),
        callbacks=callbacks,
        logger=logger_tb,
        val_check_interval=training_config.get('val_check_interval', 1.0),
        gradient_clip_val=training_config.get('gradient_clip_val', 1.0),
        accumulate_grad_batches=training_config.get('accumulate_grad_batches', 1),
        precision=training_config.get('precision', 32),
        log_every_n_steps=config.get('experiment', {}).get('log_every_n_steps', 50),
        enable_progress_bar=True,
        enable_model_summary=True,
        # detect_anomaly=True,
        # num_sanity_val_steps=0,
    )
    
    # è®­ç»ƒæ¨¡å‹
    logger.info("Starting training...")
    trainer.fit(model, data_module)
    
    # æµ‹è¯•æ¨¡å‹
    if hasattr(data_module, 'test_dataloader') and data_module.test_dataloader() is not None:
        logger.info("Starting testing...")
        # å°è¯•ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
        checkpoint_path = 'best'
        # å¦‚æœæœ‰model checkpoint callbackå¹¶ä¸”ä¿å­˜äº†æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨å®ƒ
        checkpoint_callback = None
        for callback in callbacks:
            if isinstance(callback, (ModelCheckpoint)):
                checkpoint_callback = callback
                break
        
        if checkpoint_callback and hasattr(checkpoint_callback, 'best_model_path') and checkpoint_callback.best_model_path:
            checkpoint_path = checkpoint_callback.best_model_path
            logger.info(f"Using best model for testing: {checkpoint_path}")
        
        test_results = trainer.test(model, data_module, ckpt_path=checkpoint_path)
    else:
        test_results = None
        logger.warning("No test dataloader found, skipping testing")
    
    # # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    # final_model_path = output_path / 'final_model.ckpt'
    # trainer.save_checkpoint(final_model_path)
    
    logger.info(f"Training completed! Results saved to {output_dir}")
    
    # è·å–æœ€ä½³æ¨¡å‹è·¯å¾„
    best_model_path = None
    for callback in callbacks:
        if isinstance(callback, (ModelCheckpoint)):
            if hasattr(callback, 'best_model_path') and callback.best_model_path:
                best_model_path = callback.best_model_path
                break
    
    return {
        'model': model,
        'trainer': trainer,
        'test_results': test_results,
        'output_dir': output_dir,
        'best_model_path': best_model_path
    }


def scenario_comparison_experiment(config: Dict[str, Any], data_module: MOADataModule, 
                                 output_dir: str) -> Dict[str, Any]:
    """åœºæ™¯æ¯”è¾ƒå®éªŒ"""
    
    logger.info("Starting scenario comparison experiment...")
    
    results = {}
    scenarios = config['evaluation']['scenarios']
    
    for scenario in scenarios:
        logger.info(f"Training model for scenario: {scenario}")
        
        # ä¿®æ”¹é…ç½®ä»¥ä¸“æ³¨äºç‰¹å®šåœºæ™¯
        scenario_config = config.copy()
        scenario_config['experiment']['name'] = f"scenario_{scenario}"
        
        # è®­ç»ƒæ¨¡å‹
        scenario_output_dir = os.path.join(output_dir, f"scenario_{scenario}")
        scenario_results = train_moa_model(
            scenario_config, data_module, scenario_output_dir, f"scenario_{scenario}"
        )
        
        results[scenario] = scenario_results
    
    # æ¯”è¾ƒç»“æœ
    logger.info("Scenario comparison results:")
    for scenario, result in results.items():
        logger.info(f"  {scenario}: {result.get('test_results', 'No test results')}")
    
    return results



def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Train MultiModal MOA Prediction Model - Stage 2')
    
    parser.add_argument('--config', type=str, 
                       default='models/distangle_multimodal/config_distatngle_multimodal_moa_predictor2.yaml')
    parser.add_argument('--data_dir', type=str,
                       default='preprocessed_data/CDRP-BBBC047-Bray/nvs_addnegcontrue')
    parser.add_argument('--output_dir', type=str, 
                       default='results_distangle/multimodal_stage2')
    parser.add_argument('--experiment_name', type=str, 
                       default='multimodal_moa_experiment')
    parser.add_argument('--stage1_checkpoint', type=str, default='')
    parser.add_argument('--stage2_training', action='store_true')
    parser.add_argument('--freeze_layers', type=str, nargs='*', default=None)
    parser.add_argument('--split_index', type=int, default=0)
    
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    if args.stage2_training or args.stage1_checkpoint:
        if args.stage1_checkpoint:
            config.setdefault('training', {})['stage1_checkpoint_path'] = args.stage1_checkpoint
            logger.info(f"Stage 2 training with checkpoint: {args.stage1_checkpoint}")
        
        if args.freeze_layers:
            config.setdefault('training', {}).setdefault('freeze_layers', {})['enabled'] = True
            config['training']['freeze_layers']['layers'] = args.freeze_layers
            logger.info(f"Freezing layers: {args.freeze_layers}")
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    data_config = config.get('data', {})
    pl.seed_everything(seed=data_config.get('random_seed', 2025), workers=True)
    data_module = MOADataModule(
        data_dir=args.data_dir,
        dataset_name=data_config['dataset_name'],
        batch_size=data_config.get('batch_size', 32),
        num_workers=data_config.get('num_workers', 4),
        pin_memory=data_config.get('pin_memory', True),
        split_strategy='plate',
        train_split=data_config.get('train_split', 0.8),
        val_split=data_config.get('val_split', 0.1),
        test_split=data_config.get('test_split', 0.1),
        preload_features=data_config.get('preload_features', True),
        preload_metadata=data_config.get('preload_metadata', True),
        return_metadata=data_config.get('return_metadata', True),
        feature_groups_only=data_config.get('feature_groups_only', None),
        metadata_columns_only=data_config.get('metadata_columns_only', None),
        device=data_config.get('device', 'cpu'),
        moa_column=data_config.get('moa_column', 'Metadata_moa'),
        save_label_encoder=data_config.get('save_label_encoder', True),
        feature_group_mapping=data_config.get('feature_group_mapping', None),
        normalize_features=data_config.get('normalize_features', False),
        normalization_method=data_config.get('normalization_method', 'standardize'),
        exclude_modalities=data_config.get('exclude_modalities', None),
        save_scalers=data_config.get('save_scalers', True),
        random_seed=data_config.get('random_seed', 2025),
        multi_label_classification=data_config.get('multi_label_classification', False),
        label_separator=data_config.get('label_separator', '|'),
        min_label_frequency=data_config.get('min_label_frequency', 1)
    )
    
    logger.info(f"MultiModal MOA Prediction Model Training - Stage 2")
    logger.info(f"Data dir: {args.data_dir}")
    logger.info(f"Output dir: {args.output_dir}")
    
    try:
        data_module.setup(split_index=args.split_index)
        results = train_moa_model(config, data_module, str(output_dir), args.experiment_name)
        logger.info(f"Training completed!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise


if __name__ == '__main__':
    main()